Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.6
Creation-Date: 2025-07-31T16:28:45+03:00

====== Ollama ======

[ @debian @install @ai @ollama ]

''curl -fsSL https://ollama.com/install.sh | sh''

===== Запустить =====

Вероятно, сервер Ollama будут запущен сразу же после установки. Проверить:

''ps aux | grep ollama''

или

''sudo systemctl status ollama''

Если да, то всё ок.

Если нет, то запускать локальный сервер Ollama надо так:

'''
ollama serve
'''

Тут именно ''serve'', а не ''server''.

===== Подавить сервис =====

Отныне и впредь Ollama будет запускаться как сервис вместе с системой. В норме она занимает 40 Мб и вроде бы незаметна. Если надо вызывать её только по собственному запросу, сервис надо остановить и замаскировать:

1. ''sudo systemctl stop ollama.service''
2. ''sudo systemctl disable ollama.service''
3. ''sudo rm /etc/systemd/system/ollama.service''
4. ''sudo systemctl mask ollama.service''

Проверить состояние:

''sudo systemctl status ollama''

Отныне запускать Ollama только вручную через nohup, который отвязывает запуск команды от консоли:

''nohup ollama serve > ~/.ollama/log.txt 2>&1 &''

Можно даже отказаться от сбора логов сервера:

''nohup ollama serve > /dev/null 2>&1 &''

Остановить сервер Ollama:

''pkill -f "ollama serve"'' 

Есть смысл сделать алиасы в ''.bashrc'' для этих команд.

===== Рабочие файлы =====

Все запросы юзера сохраняются в файле ''~/.ollama/history''

===== Куда сохраняются файлы LLM =====

В принципе всё должно закачиваться в ''~/.ollama/models/blobs/'' 

Там хранятся
* весовые файлы (.bin, .safetensors);
* конфиги (config.json);
* токенизаторы (tokenizer.json, merges.txt, vocab.json);
* quant-файлы (gguf, ggml, и т.п.) — если модель используется с llama.cpp и его производными.

Но изначально все файлы сохраняются в ''~/.cache/huggingface''

В случае нарочного или случайного сбоя скачивания, LLM начнет работать на тех файлах, которые будут в наличии в каталоге с кэшем. Если скачивание будет полноценным, то файлы модели будут перенесены в каталог ''~/.ollama/''

===== Поменять каталог сохранения всех LLM =====

Переместить исходный каталог Ollama в новое место:

''mkdir -p /mnt/llms; mv ~/.ollama/models /mnt/llms/''

Через переменную окружения указать Ollama, где искать модели. Добавить в файл ''~/.bashrc'' новую строку:

''export OLLAMA_MODELS=/mnt/llms/models''

Важно: переменная ''OLLAMA_MODELS'' должна указывать на подкаталог ''/models'' — там Ollama будет создавать  подкаталоги для каждой LLM.

См. [[Software:Ollama]]
