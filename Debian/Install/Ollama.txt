Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.6
Creation-Date: 2025-07-31T16:28:45+03:00

====== Ollama ======

[ @debian @install @ai @ollama ]

''curl -fsSL https://ollama.com/install.sh | sh''

Файлы будут автоматически скачаны и уложены в ''/usr/local/lib/ollama''

===== Остановить systemd-сервис =====

Также будет создан systemd сервис, который будет автоматически запускать сервер ollama при загрузке системы. Если это не нужно — остановить сервис и удалить два его файла:

''sudo systemctl stop ollama.service''
''sudo systemctl disable ollama.service''
''sudo rm /etc/systemd/system/default.target.wants/ollama.service''
''sudo rm /etc/systemd/system/ollama.service''
''sudo systemctl daemon-reload''

Проверить:

'''
sudo systemctl status ollama
'''

Ожидаемый ответ: ''Unit ollama.service could not be found.''

===== Запустить Ollama =====

В принципе сервер Ollama будут запущен сразу же после установки. Проверить:

''ps aux | grep ollama''

Включить локальный сервер Ollama:

'''
ollama serve
'''

Тут именно “''serve''”, а не “''server''”.

Разумно запускать Ollama из консоли только вручную, через nohup, который отвязывает запуск команды от консоли:

''nohup ollama serve > ~/.ollama/log.txt 2>&1 &''

Можно даже отказаться от сбора логов сервера:

''nohup ollama serve > /dev/null 2>&1 &''

Если сервис запущен (работает как сервер в фоне), то всё ок.

===== Остановить сервер Ollama =====

''pkill -f "ollama serve"'' 

===== Алиасы в ''.bashrc'' =====

{{{code: id="ollama_aliases" lang="sh" linenumbers="True"
    # ==== LLM ollama ===

ollama_start() {
    if pgrep -x ollama >/dev/null; then
        echo "ollama уже запущен"
    else
        nohup ollama serve >/dev/null 2>&1 &
        echo "ollama is started"
    fi
}

ollama_stop() {
    # находим действующий PID процесса ollama serve и убиваем его
    PIDS=$(pgrep -f "ollama serve")
    if [ -z "$PIDS" ]; then
        echo "ollama is not running"
    else
        kill $PIDS
        echo "ollama is stopped"
    fi
}

    # Запуск Ollama
alias ollama+='ollama_start'

    # Остановка Ollama
alias ollama-='ollama_stop'
}}}

===== Рабочие файлы =====

Все запросы юзера сохраняются в файле ''~/.ollama/history''

===== Куда сохраняются файлы LLM =====

В принципе всё должно закачиваться в ''~/.ollama/models/blobs/'' 

Там хранятся
* весовые файлы (.bin, .safetensors);
* конфиги (config.json);
* токенизаторы (tokenizer.json, merges.txt, vocab.json);
* quant-файлы (gguf, ggml, и т.п.) — если модель используется с llama.cpp и его производными.

Но изначально все файлы сохраняются в ''~/.cache/huggingface''

В случае нарочного или случайного сбоя скачивания, LLM начнет работать на тех файлах, которые будут в наличии в каталоге с кэшем. Если скачивание будет полноценным, то файлы модели будут перенесены в каталог ''~/.ollama/''

===== Поменять каталог сохранения всех LLM =====

Переместить исходный каталог Ollama в новое место:

''mkdir -p /mnt/llms; mv ~/.ollama/models /mnt/llms/''

Через переменную окружения указать Ollama, где искать модели. Добавить в файл ''~/.bashrc'' новую строку:

''export OLLAMA_MODELS=/mnt/llms/models''

Важно: переменная ''OLLAMA_MODELS'' должна указывать на подкаталог ''/models'' — там Ollama будет создавать  подкаталоги для каждой LLM.

См. [[Software:LLM:Ollama]]
