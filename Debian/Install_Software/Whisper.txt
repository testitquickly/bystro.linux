Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.6
Creation-Date: 2024-12-01T00:30:56+02:00

====== Whisper ======

[ @debian @install @sound @openai @whisper ]

===== Если есть видеокарта GPU с CUDA =====

Можно запустить все вычисления на видеокарте. Но это должна быть «свежая» видеокарта.

1. должны быть установлены драйверы для системы — см. [[Nvidia]]

2. должна быть известна версия CUDA, которая совместима с установленной системой

''nvidia-smi''

В ответе посмотреть верхнюю строку:

''Driver Version: 535.216.01   CUDA Version: 12.2'' 

3. нужен Python и соответствующий virtual environment

''sudo apt install python3.11-venv''

Сделать каталог для всего этого дела

''mkdir ~/workspace/Whisper/ && cd ~/workspace/Whisper/''

Создать virtual environment для проекта

''python3 -m venv Whisper_env''

Активировать его

''source Whisper_env/bin/activate''

Для выхода из виртуального окружения надо выполнить в том же каталоге команду ''deactivate''

Важно и неочевидно: нельзя __перемещать__ каталог ''whisper_env'', по новому адресу не поднимется вирутальное окружение. Если очень надо его переместить, то следует пересоздать новый „virtual environment” для проекта в том же каталоге.

4. снять файлы Whisper из git и установить Whisper и все зависимости

''pip install git+https://github.com/openai/whisper.git''

5. проверить, какие версии CUDA поддерживает установленная версия ''torch''

Зайти в интерпретатор python (Note: выход из консольного интерпретатора по ''Ctrl+D'')

''python3''

Выполнить последовательно

'''
>>> import torch
'''
''>>> print(torch.cuda.is_available())''

Ожидаем: True

'''
>>> print(torch.cuda.get_device_name(0)) 
'''

Ожидаем: NVIDIA GeForce __GTX 1050 Ti__ (модель установленной видеокарты)

''>>> print(torch.version.cuda)'' 

Ожидаем: версию уже установленной CUDA из версии Nvidia — 12.2 

Если ответ другой (например, 12.4) — это означает, что из-за разницы в версиях ПО Whisper не сможет делать расчёты на видеокарте. Скорее всего причина в том, что видеокарта «постаревшая». Так или иначе, рекомендую запускать Whisper на CPU.

В принципе можно сделать свою сборку PyTorch под более старую версию CUDA — долго, и не факт, что получится правильно всё сделать без опыта.

Или можно сделать новое виртуальное окружение Python с версией пониже — у меня Python 3.11, и сделаем downgrade до Python 3.10 — и установить там PyTorch версии 12.1 — это ниже моей 12.2 и это должно сработать, потому что у CUDA есть обратная совместимость. 

Я сделал второе и Whisper запустился, но почти сразу появилось множеством сообщений о проблемах настройки окружения, которые надо понимать и уметь решать.

==== Пример (неудачного) решения ====

1)
Установить Python 3.10 в систему:

''wget https://www.python.org/ftp/python/3.10.9/Python-3.10.9.tar.xz -P /home/hdd/PythonSource && cd /home/hdd/PythonSource''

и дальше по схеме [[Software:Python#Установить отдельную версию Python]]

2) 
Создать новое виртуальное окружение с Python 3.10 и CUDA 12.2:

''python3.10 -m venv Whisper_cuda_12_2_env''

Активировать его

''source Whisper_cuda_12_2_env/bin/activate''

3)
Выполнить в нём установку ''Whisper'' + ''torch'' версии 12.1 (которая ниже моей 12.2). Сперва надо установить несколько компонентов отдельно (последствия понижения версионности ПО), затем всё остальное:

''pip install numba; pip install tiktoken; pip install more-itertools; pip install git+https://github.com/openai/whisper.git torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121''

4)
Тут же, в виртуальном окружении зайти в интерпретатор python (Note: выход из консольного интерпретатора по ''Ctrl+D'')

''python3''

Выполнить последовательно

'''
>>> import torch
'''
''>>> print(torch.cuda.is_available())''

Ожидаем: True

'''
>>> print(torch.cuda.get_device_name(0))
'''

Ожидаем: NVIDIA GeForce __GTX 1050 Ti__ (модель установленной видеокарты)

''>>> print(torch.version.cuda)''

Ожидаем: версию 12.1. 

В будущем обновлять всё это вряд ли будет возможным. 

===== Настроить Whisper =====

Whisper нагружает CPU очень агрессивно, на уровне 97-101%, даже если ему выделить только одно ядро процессора. Это разгоняет ВЕСЬ камень до +80°C, независимо от системы его охлаждения. Для компьютера в крупном корпусе это не так, чтобы «ой мама!», но всё-таки… и вентиляторы загудят. А для тесного ноутбука это прям заметно много. 

Можно «замедлить» Whisper комплексно:
* использовать модели tiny или small
* ограничить количество задействованных ядер процессора (3 штуки)
* ограничить число потоков для библиотек нейросети (BLAS, MKL, OpenMP)
* отключить вычисления в формате float16 если расчет происходит только на CPU

Модели Tiny или Small хуже распознают речь, поэтому нет, оставим Medium. Остальные ограничения заметно придушат производительность нейросети, но это означает, что Whisper всего лишь будет работать чуть дольше, и это ок. Важно то, что эти настройки снизили рабочую температуру процессора с +88°C до +70°C. 

==== Работа на CPU ====

У процессора может быть несколько физических ядер (threads), и над ними несколько виртуальных (hyperthreads). В настройке Whisper подразумевается использование //только// физических ядер.

Узнать, сколько всего ядер на процессоре компьютера:

'''
nproc
'''

Пример ответа: ''16''

Узнать, сколько физических ядер: 

''cat /proc/cpuinfo | grep 'core id' | sort -u | wc -l''

или

''grep -m1 'cpu cores' /proc/cpuinfo''

Пример ответа: ''8''

Задать количество используемых физических ядер процессора:

''--threads 3''

Чем меньше — тем медленнее и тише.

=== Ограничить число потоков вычислений ===

Whisper использует PyTorch, который может задействовать BLAS, MKL, OpenMP — технологии для ускорения вычислений в математических и научных задачах. Это ускоряет обработку, но может занять все ядра процессора, вызывая перегрузку.

1. BLAS (Basic Linear Algebra Subprograms) — библиотека базовых операций линейной алгебры (матрицы, векторы).
2. MKL (Math Kernel Library) — библиотека Intel, включает BLAS, LAPACK, FFT и другие алгоритмы.
3. OpenMP (Open Multi-Processing) — API для параллельного выполнения кода на CPU. Позволяет использовать многоядерность для ускорения вычислений.

Проверить их настройки по-умолчанию:

''echo $OPENBLAS_NUM_THREADS; echo $MKL_NUM_THREADS; echo $OMP_NUM_THREADS; echo $NUMEXPR_NUM_THREADS''

Если в ответ пустота, это значит, что Whisper будет использовать их без ограничений.

Можно регулировать многопоточность этих вычислений через объявление количества используемых ядер в четырех переменных, например „''2''”. 

Задать „''1''” для принудительного перевода в режим однопоточности:

'''
export OPENBLAS_NUM_THREADS=1
export MKL_NUM_THREADS=1
export OMP_NUM_THREADS=1
export NUMEXPR_NUM_THREADS=1
'''

=== Отключить вычисления в формате float16 ===

Это нужно, только если транскрибирование выполняется только на CPU.

Добавить в цепочку настроек Whisper при запуске параметр

''--fp16 False'' 

==== Работа на GPU ====

В случае в видеокартой от Nvidia надо сперва установить драйвера, которые ей нужны, см. [[Nvidia]] и установить в виртуальном окружении Python свежую версию PyTorch, см. [[Whisper]]

Затем установить утилиту для мониторинга GPU

''sudo apt install nvtop''

После запуска Whisper вызвать в отдельной консоли ''nvtop'', а в другой ''htop'' и смотреть на графики. 

Также можно сделать

''nvidia-smi''

Если в таблице будет строка вроде 

''|    0   N/A  N/A     27061      C   ...ace/whisper/whisper_env/bin/python3     5416MiB'' 

то всё ок.

После этого рассмотреть настройки Whisper для CPU (кроме подавления вычислений в формате float16).

===== Обновить Whisper =====

Узнать, если есть обновления — https://github.com/openai/whisper/releases/latest

Whisper установлен как пакет Python, поэтому надо а) поднять виртуальное окружение и уже там б) узнать установленную версию

''pip show openai-whisper | grep 'Version'''

и если решено обновлять, то:

''pip install --upgrade openai-whisper''

Также можно сделать

''pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git''

См. [[Software:Whisper]]
