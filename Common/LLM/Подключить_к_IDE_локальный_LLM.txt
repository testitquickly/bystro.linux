Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.6
Creation-Date: 2025-08-18T21:23:17+03:00

====== Подключить к IDE локальный LLM ======

[ @ide @llm ]

[[Software:VS Code]] (или его аналог — VSCodium) настроен на то, чтобы юзер подключил GitHub Copilot, привык к его автоподсказкам и auto fix и подписался на сервис на год вперед. $100 в год.

Но Copilot хорошо и быстро пишет/дополняет код, а в анализе и объяснениях он слабоват. А обсуждать архитектуру, алгоритмы, оптимизацию и дебаг в ходе разработки — is a must. И через какое-то время юзер захочет подключить в VS Code другие LLM (chatGPT, CLaude, Sonnet, DeepSeek и тд), а это +$100 или $200. И если разработка идет в группе, то надо раздать LLM каждому. Какое-то время все будут переключаться между IDE и браузером, но…

В IDE можно подключить локальные LLM, причем разные — и для code completition, и для rationament. И запускать это всё можно

* на своем компьютере c сильным GPU;
* на своем компьютере, только на CPU;
* на отдельном сервере, даже без GPU.

Учесть, что

* при переходе на локальные LLM повышается приватность, бесплатность, повышенный контекст и гибкость настроек;
* качество работы локальных LLM всегда будет ниже действующих облачных. Непредсказуемо, но иногда эта разница будет заметной;
* локально работающие модели нагружают сразу всё, что можно, и под нагрузкой будет перегреваться и выть любой компьютер. Надо 
	* аккуратно выбирать используемые модели по количеству параметров и по степени [[Software:LLM#квантование|квантования]] весов нейросети,
	* и тонко настраивать их возможности и ограничения, чтобы нагрузка соответствовала необходимому запросу.

==== Модели для кода ====

Примеры для августа 2025-го.

=== Лёгкие и быстрые ===

* DeepSeek Coder 6.7B — сейчас топ среди компактных для кода
* Qwen2.5-Coder 7B — сильнее в объяснениях
* Code LLaMA 7B — стабильный, староват

=== Средние ===

* DeepSeek Coder 14B — близок к Copilot, понимает большие файлы.
* Qwen2.5-Coder 14B

=== Тяжёлые ===

Нужно 24+ GB VRAM (или CPU с 64+ GB RAM, но медленно).

* DeepSeek Coder 33B
* Mixtral 8x7B Instruct

==== Общая схема ====

1. Установить локальную LLM через [[Software:Ollama]] (good enough для начала) или
	b. [[https://github.com/oobabooga/text-generation-webui|text-generation-webui]] — много возможностей, ручная настройка 
	c. [[https://github.com/vllm-project/vllm|vLLM]] — от ученых из Беркли, для продвинутых, быстрая подача токенов
	d. [[https://lmstudio.ai/|lmstudio.ai]] — free for home use 
2. Стянуть через Ollama модель для подсказок (Code LLaMA, DeepSeek Coder, StarCoder, Qwen-Coder)
3. Стянуть через Ollama модель для анализа и объяснений ()
4. Установить в VS Code open-source AI code assistant плагин “Continue”
5. Настроить “Continue” так, чтобы VS Code отправлял запросы на локальный сервер

==== Локальный сервер ====

1. Установить локальную LLM через [[Software:Ollama]] 



==== Удаленный сервер (CPU) ====

Не каждый выделенный сервер оснащён выделенной видеокартой, поэтому модели будут работать на CPU. Это сразу означает «медленнее», но не так, чтобы критично. Главное то, что всё это можно настраивать — приоритет скорости ответов или их точности.

У меня сервером будет отдельно стоящий слабый офисный Beelink Mini S12 Pro (Intel 12th Gen N100 (4C/4T, up to 3.4GHz), 16GB DDR4, 500GB SSD) с Debian без GUI. В нём нет выделенной GPU, поэтому надо тонко настроить llm-окружение под его производительность.
