Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.6
Creation-Date: 2025-08-18T22:55:01+03:00

====== Терминология ======

[ @llm @ai ]

===== Матрица =====

Прямоугольная таблица чисел, у которой есть строки (rows) и столбцы (columns).

Пример матрицы 3×3:

''[ 1  2  3 ]''
''[ 4  5  6 ]''
''[ 7  8  9 ]''

Это 2-мерный массив (2D), он также называется тензор второго порядка (2D).

В LLM веса хранятся именно как тензоры — иногда это простые 2D-матрицы весов слоя, иногда более многомерные структуры для оптимизации.



===== Тензор =====

Это обобщение матрицы на больше измерений (3D, 4D, 5D…). Тензор третьего и выше порядка — массив, у которого больше двух измерений.

Например, 3D-тензор можно представить как набор матриц, сложенных в “куб”:

'''
[
  [ [1, 2], [3, 4] ],
  [ [5, 6], [7, 8] ]
]
'''

У тензора есть размерность.

===== Размерность =====

Dimension. Это количество измерений (осей), которые нужны, чтобы однозначно описать положение каждого элемента в массиве. Другими словами, размерность — указание на то, сколько координат нужно учесть, чтобы найти элемент в матрице чисел.

То есть, размерность (dimension) говорит, сколько осей есть в матрице.

У тензора есть «размерность массива чисел по осям», которую для краткости называют по-английски shape. Shape показывает размер по каждой оси матрицы внутри тензора.

Пример: как выглядит 3D-тензор с формой (shape) ''(2, 3, 4):''

* dimension (размерность) = ''3'' (''3D'', три оси: «глубина», строки, столбцы)
* shape = ''(2,3,4)'' показывает размер по каждой оси:
	* Первая ось (глубина) = 2 матрицы
	* Вторая ось (строки) = 3 строки в каждой матрице
	* Третья ось (столбцы) = 4 числа в каждой строке

Матрица 1:
''[''
	''[1, 2, 3, 4],''
	''[5, 6, 7, 8],''
	''[9,10,11,12]''
'']''

Матрица 2:
''[''
	''[13,14,15,16],''
	''[17,18,19,20],''
	''[21,22,23,24]''
'']''



контексте тензоров термин dimensions часто используют почти как rank — то есть количество осей (измерений).

Разница между терминами:

dimension / dimensions — «количество осей», иногда говорят «размерность тензора».

rank — технический термин, официально означает порядок тензора, то есть тоже количество осей.

shape — длина по каждой оси (например, (2, 3, 4)), а dimension / rank = длина этого кортежа = 3.

Пример:

Тензор с shape = (5, 10, 3)

dimensions / rank = 3

shape = (5, 10, 3)

То есть: shape показывает размеры по осям, а dimensions/rank — сколько этих осей есть.



Скаляр: 42 → shape = () → размерность = 0 (нет осей).
Размерность скаляра — ''0''. описывают как shape (например, shape=(2,2,2) — две матрицы 2×2).

порядок тензора (rank), равный длине (shape).




Примеры:


Вектор: ''[1, 2, 3]'' → shape = (3,) → размерность = 1 (одна ось — длина 3).

Матрица: ''[1, 2, 3], [4, 5, 6]'' → shape = (2, 3) → размерность = 2 (ось строк и ось столбцов).

3D-тензор: shape = ''(2, 2, 2)'' → размерность = 3 (ось “глубины”, ось строк, ось столбцов).


Например, для матрицы нужна пара координат (row, col), а для 3D — уже тройка (depth, row, col).

===== Скаляр =====

Тензор нулевого порядка (одно число).

Пример:

''42''

Shape — пустой, (), нет измерений.

===== Вектор =====

Одномерный массив чисел. Тензор первого порядка (1D).

Пример:

''[1, 2, 3]''

Shape: (3,) — одно измерение длиной 3.


===== Floating Point =====

Один из форматов хранения чисел с плавающей запятой (в Computer Science принято говорить «с плавающей точкой»). 

Формат этот называется FP — Floating Point (плавающая точка).

Формат FP состоит из трёх частей:

* знак (1 бит) — положительное или отрицательное число;
* порядок (экспонента) — сдвигает “запятую” влево или вправо;
* мантисса — собственно цифры числа с определённой точностью.

Есть несколько видов FP:

* FP16 (float16, half precision) — 16 бит (2 байта) на число, точность около 3–4 десятичных знаков.
* BF16 (bfloat16) — тоже 16 бит, но с другой разрядностью порядка и мантиссы (лучше для нейросетей).
* FP32 (float32) — 32 бита (4 байта) на число, точность около 7–8 десятичных знаков.

Чем меньше бит выделяется на число, тем меньше памяти нужно для его хранения и использования, но и тем меньше точность вычислений с ним.

===== Веса =====

В LLM веса (weights) — это просто огромные матрицы чисел в FP.

===== Параметры =====

Параметр = одно число (обычно вес) в матрице или тензоре нейросети.

Это обучаемые числа (веса), которые определяют, как модель обрабатывает входные данные и генерирует ответ.

**Обучаемые** — числа, которые не заданы заранее вручную, а подбираются автоматически в процессе обучения модели, чтобы она выполняла задачу как можно лучше.







Эти параметры хранятся в виде FP32, FP16 или в виде квантованных значений.





При обучении параметры подбираются, чтобы минимизировать ошибку на тренировочных данных.

Структурно:

LLM состоит из слоёв (входной, трансформеры, выходной).

Каждый слой имеет матрицы весов (например, 4096×4096), которые умножаются на вектор входных данных.

Каждый элемент в этих матрицах — и есть параметр.

Пример для 33B:

“33B параметров” = 33 000 000 000 весов (чисел).

Если хранить их в FP16 (2 байта каждое) → ~66 ГБ.

Если в INT4 (0.5 байта каждое) → ~16.5 ГБ.

То есть количество параметров напрямую связано с объёмом памяти и скоростью вычислений — чем их больше, тем мощнее, но и тяжелее модель.

===== Квантование =====

Это сжатие весов (и иногда вычислений) модели до меньшего числа бит. Размеры современных моделей огромны, не каждый CPU может их переварить. Через установку степени квантования можно принудительно «сжать» модель, чтобы уменшить объемы вычислений. 

Модель на 33B параметров в FP16 весит ''33 млрд × 2 байта ≈ 66 ГБ''

В FP32 это было бы около 132 ГБ — на обычный ПК не влезет.

Что даёт 4-битное квантование


При 4 битах (0.5 байта на параметр) вес модели:

33 млрд × 0.5 байта ≈ 16.5 ГБ

Это уже можно запустить на CPU с большим объёмом RAM.

Почему на CPU всё равно медленно

Квантованные веса перед вычислением часто расквантовываются обратно в 16/32-бит, чтобы процессор мог умножать их на активации — это добавляет вычислительных шагов.

CPU не имеет таких тензорных блоков, как GPU, поэтому матричные операции идут медленнее.

Даже с 4-битным сжатием поток данных (memory bandwidth) всё ещё огромный.

Вывод4-битное квантование позволяет уместить модель в память, но не ускоряет CPU — наоборот, иногда чуть замедляет из-за расквантования на лету.

Исходно они представлены в размере 16 или 32 бит с плавающей точкой.

Можно запустить модель на 33B параметров на CPU с 4-битным квантованием, но она будет работать медленнее. Поэтому при выборе модели стоит учитывать как количество параметров, так и степень квантования.

FP32 — веса хранятся в 32-битном формате с плавающей точкой (полная точность, но большой объём памяти).

FP16 / BF16 — 16-битная плавающая точка (в 2 раза меньше памяти, немного меньше точность).

INT8 — целые числа 8 бит (ещё меньше памяти, но уже заметная потеря точности).

INT4 — целые числа 4 бита (в 8 раз меньше памяти, чем FP32, но точность ещё сильнее падает).



Например:


Объём весов без квантования

