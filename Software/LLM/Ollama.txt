Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.6
Creation-Date: 2025-07-31T16:22:45+03:00

====== Ollama ======

[ @ai @ollama ]

Ollama это утилита, которая управляет установкой и управлением множества LLM-моделей (ИИ, см. [[Common:LLM]]) через управление llama.cpp — это библиотека, на которой основаны многие LLM-инструменты. Условный “LLM Docker”.

Список доступных для ollama LLM — https://ollama.com/search

Она проста в использовании: запускается одной командой, сама грузит модель и запускает её на локалке, и уже к её серверу нему подключаются утилиты вроде [[Software:aider]].

===== Установить =====

* [[Debian:Install:Ollama]]

===== Выбрать модель LLM =====

Надо точно знать параметры выбираемой модели. Та же ''llama3'' представлена и в виде слабой ''llama3:4b'' (за буквой “b” в названиях LLM подразумевается «//…миллиардов параметров//»), и в виде здоровой ''llama3:70b''., которую не каждый компьютер потянет без большого количества свободной памяти, большого hdd и хорошего охлаждения.

Некоторые LLM могут сносно работать только на CPU, другие без GPU или не поднимутся, или загубят процессор.

Для проверки Ollama достаточно запустить старую ''llama3'', она хоть и бестолковая во многих смыслах, но нетребовательна к ресурсам. 

==== Модели «для CPU» ====

* phi-3:mini
* gemma:2b	///Google//
* codegemma:2b	///Google//
* deepseek-coder:1.3b
* mistral
* llama3:8b

=== Модели «для GPU» ===

* mixtral
* deepseek-coder:33B
* wizardcoder:15b
* llama3:70b

===== Запустить LLM через Ollama =====

Скачать и запустить через ''Ollama'' бесплатную LLM-модель «''llama3''»:

''ollama run llama3''

Вес модели от 4 до 8 ГБ. 

Сразу после скачивания будет запущена интерактивная сессия (выход по ''Ctrl+D'' или команде ''/bye'') и поскольку это сервер, к которому мы подключается через ssh, произойдет обмен ключами доступа, которые следует сразу принять, и они будут сохранены в каталог ''~/.ollama'',

В том же окно откроется сессия общения (''>>>''), в которой можно или задавать вопросы IRL, или вызывать управляющие команды:

''/set            Set session variables''
  ''/show           Show model information''
  ''/load <model>   Load a session or model''
  ''/save <model>   Save your current session''
  ''/clear          Clear session context''
  ''/bye            Exit''
  ''/?, /help       Help for a command''
  ''/? shortcuts    Help for keyboard shortcuts''

''Use """ to begin a multi-line message.''

''>>> /show''

''Available Commands:''
  ''/show info         Show details for this model''
  ''/show license      Show model license''
  ''/show modelfile    Show Modelfile for this model''
  ''/show parameters   Show parameters for this model''
  ''/show system       Show system message''
  ''/show template     Show prompt template''

Возраст модели, количество её параметров и тд можно узнать через команду ''/show info'' Пример ответа:

''Model''
	''architecture        llama''    
	''parameters          8.0B''     (восемь миллиардов)
	''context length      8192''     
	''embedding length    4096''     
	''quantization        Q4_0''     

  ''Capabilities''
	''completion''    

  ''Parameters''
	''num_keep    24''                       
	''stop        "<|start_header_id|>"''    
	''stop        "<|end_header_id|>"''      
	''stop        "<|eot_id|>"''             

  ''License''
	''META LLAMA 3 COMMUNITY LICENSE AGREEMENT''             
	''Meta Llama 3 Version Release Date: April 18, 2024''    
	''...''    

…это норм.

Проверить, какие модели уже скачаны через ''ollama'':

''ollama list''

Пример ответа:

''NAME             ID              SIZE      MODIFIED''    
''llama3:latest    365c0bd3c000    4.7 GB    9 hours ago''  

===== Настройки модели =====

Посмотреть текущий системный запрос:

''/show system''

Задать свой:

''/set system parameter''

Пример (не факт, что сработает):

//I need to learn Linux commands. You are my Linux tutor. Your task is to name a command and wait for my answer. If my answer is be right, give me a cookie. If not — not. Give me three command names in total, ask one by one and check my answer for each one.//

===== Запросы через API =====

По умолчанию Ollama ожидает запросы на порту ''11434''. 

API чата совместим с OpenAI, поэтому URL для запроса будет выглядеть так:

http://localhost:11434/api/v1/chat/completions

Пример запроса через curl:

''curl http://localhost:11434/v1/chat/completions -d '{''
''"model": "gemma3:12b",''
''"messages": [''
''{''
''"role": "user",''
''"content": "Who created Linux kernel? Answer only name"''
''}''
''],''
''"stream": false''
''}' | json_pp''

===== Удалить модель =====

Посмотреть список установленных:

''ollama list''

Сперва надо погасить службу Ollama:

''sudo systemctl disable ollama''

Удалить определенную модель:

''ollama rm llama3'' 
