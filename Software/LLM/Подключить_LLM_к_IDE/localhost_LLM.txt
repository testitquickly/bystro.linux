Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.6
Creation-Date: 2025-09-04T02:33:09+03:00

====== localhost LLM ======

[ @ide @llm @localhost @ollama ]

Технически говоря, это называется «поднять локальный сервер».

1. Установить [[Ollama]] 
2. Стянуть и запустить через Ollama модель для подсказок в коде. Для примера взять маленькую ''qwen2.5-coder:3b''

''ollama run qwen2.5-coder:3b''

Отредактировать файл с настройками Continue:

'''
mcedit ~/.continue/config.yaml
'''

прописать настройку с точным названием используемой через ollama модели (например “''qwen2.5-coder:7b''”) и url действующего на localhost сервера ollama на стандартном порту:

{{{code: lang="yaml" linenumbers="True"
name: my-configuration
version: 0.0.1
schema: v1

models:
  - name: localhost-qwen
    provider: ollama
    model: qwen2.5-coder:3b
    apiBase: http://localhost:11434
    completionOptions:
      contextLength: 20000
      temperature: 0

chat:
  model: localhost-qwen

autocomplete:
  model: localhost-qwen
  mode: nextEdit   # можно заменить на fim, если модель поддерживает
  maxTokens: 64

edit:
  model: localhost-qwen

fix:
  model: localhost-qwen

context:
  - provider: code
  - provider: currentFile
  - provider: diff
  - provider: http
  - provider: problems
  - provider: terminal
  - provider: debugger

}}}

Запустить VSCodium (или перезапустить, если был открыт), открыть Continue Console. 

Настройки должны подхватиться автоматически: в разделе чатов появляется новая модель, в коде должны появиться подсказки.

См. далее [[Подключить LLM к IDE]]
