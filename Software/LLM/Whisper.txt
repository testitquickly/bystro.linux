Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.6
Creation-Date: 2024-11-30T21:46:21+02:00

====== Whisper ======

[ @machine_learning @openai @whisper @shortcuts ]

«Whisper» — нейронная сеть, модель для автоматического распознавания речи — automatic speech recognition (ASR) от OpenAI (есть аналоги). Может преобразовывать аудиофайлы в текст (это называется «транскрибирование»). Работает локально. Поддерживает множество языков (русский и украинский) под «MIT License». Опубликована с открытым исходным кодом. 

Что умеет:

1. Транскрибация аудио и видео — расшифровка интервью, подкастов, лекций и др. Создание субтитров
2. Голосовые помощники и чат-боты
3. Автоматизация работы с аудиоархивами — распознавание и индексирование больших аудиоколлекций и «поиск по аудиофайлам» на основе полученного текста
4. Перевод речи в текст в реальном времени
5. Лингвистический анализ и исследование речи через исследования в области диалектологии и фонетики

Описание: https://openai.com/index/whisper/

После Whisper есть смысл обработать транскрибированный текст через [[Sentence Transformer]].

===== Условности =====

Нейросети в работе вообще прожорливы, на большом количестве файлов с применением «большой» модели гудеть кулерами начнёт даже ноутбук, в котором в принципе нет вентиляторов. Разумно начинать с модели „medium” и переходить на более продвинутую только при явной необходимости. Также из сострадания к ресурсам компьютера разумно придирчиво отбирать, какие аудиофайлы действительно //надо// транскрибировать. И в принципе надо сперва настроить то, как Whisper будет использовать CPU (см. файл про его установку). 

У Whisper есть несколько моделей (уже натренированные нейросети):

* //tiny// и //base//: работают быстро, но точность распознавания очень, эээ, плохая.
* //small//: малая точность, но работает шустро. 
* //medium//: баланс между точностью и скоростью.
* //large//: высокая точность, но требует много ресурсов, сильно разгоняет даже очень мощный процессор (можно настроить балансировку работы между ядрами). Представлена в нескольких версиях: large-v1 (по-умолчанию), large-v2 и large-v3.
* //turbo//: потребляет меньше ресурсов, чем модель //large//, но заточена под скорость, а не под точность расшифровки.

Модель „small” на момент тестирования весит 461 МБ. Модель „large” [[https://github.com/openai/whisper/blob/main/README.md#available-models-and-languages|потребует]] скачать почти три гигабайта. Позже будет больше.

Каждая модель Whisper перед первым её применением будет автоматически загружена из сети и сохранена в кэше профиля пользователя: ''~/.cache/whisper/'' в файле с расширением ''.pt''

Проверить, какие модели доступны:

''import whisper''
''print(whisper._MODELS.keys())''

Ожидаемый ответ:

''dict_keys(['tiny', 'base', 'small', 'medium', 'large', 'large-v1', 'large-v2', 'large-v3'])''

===== Установить Whisper =====

* [[Debian:Install:Whisper]]

===== Настроить Whisper =====

Whisper нагружает CPU очень агрессивно, на уровне 99-101%, даже если ему выделить только одно ядро процессора. Это разгоняет ВЕСЬ камень до +80°C, независимо от системы его охлаждения. Для компьютера в крупном корпусе это не так, чтобы «ой мама!», но всё-таки… и вентиляторы загудят. А для тесного ноутбука это прям заметно много. 

Можно «замедлить» Whisper комплексно:

* использовать модели tiny или small
* ограничить количество задействованных ядер процессора (3 штуки)
* ограничить число потоков для библиотек нейросети (BLAS, MKL, OpenMP)
* отключить вычисления в формате float16 если расчет происходит только на CPU
* наколдовать себе балансировщик нагрузки на процессор (по-умолчанию, даже если в работе полностью задействована выделенная видеокарта, на CPU происходит интенсивная работа, при этом полностью нагружено только одно ядро, даже если их множество, отчего процессор изрядно нагревается; принудительный перевод нагрузки между ядрами помогает убрать нагрев с одной точки на камне, снижая общую рабочую температуру до условных +75°C, а нагрузка до +80°C остается краткосрочной, только после перехода к транскрибированию следующего файла) 

Модели Tiny или Small хуже распознают речь, поэтому нет, оставим Medium. Остальные ограничения заметно придушат производительность нейросети, но это означает, что Whisper всего лишь будет работать чуть дольше, и это ок. Важно то, что эти настройки снизили рабочую температуру процессора с +88°C до +70°C. 

==== Работа на CPU ====

У процессора может быть несколько физических ядер (threads), и над ними несколько виртуальных (hyperthreads). В настройке Whisper подразумевается использование //только// физических ядер.

Узнать, сколько у процессора всего ядер:

'''
nproc
'''

Пример ответа: ''16''

Узнать, сколько у процессора физических ядер: 

''grep -m1 'cpu cores' /proc/cpuinfo''

или так

''cat /proc/cpuinfo | grep 'core id' | sort -u | wc -l''

Пример ответа: ''8''

Задать количество используемых физических ядер процессора:

''--threads 3''

Чем меньше ядер используется — тем тише и дольше будет работать Whisper.

=== Ограничить число потоков вычислений ===

Whisper использует PyTorch, который может задействовать BLAS, MKL, OpenMP — технологии для ускорения вычислений в математических и научных задачах. Это ускоряет обработку, но может занять все ядра процессора, вызывая перегрузку.

1. BLAS (Basic Linear Algebra Subprograms) — библиотека базовых операций линейной алгебры (матрицы, векторы).
2. MKL (Math Kernel Library) — библиотека Intel, включает BLAS, LAPACK, FFT и другие алгоритмы.
3. OpenMP (Open Multi-Processing) — API для параллельного выполнения кода на CPU. Позволяет использовать многоядерность для ускорения вычислений.

Проверить их настройки по-умолчанию:

''echo $OPENBLAS_NUM_THREADS; echo $MKL_NUM_THREADS; echo $OMP_NUM_THREADS; echo $NUMEXPR_NUM_THREADS''

Если в ответ пустота, это значит, что Whisper будет использовать их без ограничений.

Можно регулировать многопоточность этих вычислений через объявление количества используемых ядер в четырех переменных, например „''2''”. 

Задать „''1''” для принудительного перевода в режим однопоточности:

'''
export OPENBLAS_NUM_THREADS=1
export MKL_NUM_THREADS=1
export OMP_NUM_THREADS=1
export NUMEXPR_NUM_THREADS=1
'''

=== Отключить вычисления в формате float16 ===

Это нужно, только если транскрибирование выполняется только на CPU.

Добавить в цепочку настроек Whisper при запуске параметр

''--fp16 False'' 

==== Работа на GPU ====

Очень разумно выполнять всю работу Whisper на относительно свежей видеокарте от Nvidia.

Для этого

1)
Должны быть установлены драйверы для установленной видеокарты — [[Debian:Install:Nvidia]]

И должна быть известна версия CUDA (Compute Unified Device Architecture — программно-аппаратная архитектура параллельных вычислений, работает на графических процессорах Nvidia), которая совместима с установленной системой

''nvidia-smi''

В ответе посмотреть верхнюю строку:

''Driver Version: 535.216.01   CUDA Version: 12.2'' 

2)
Проверить, какие версии CUDA поддерживает установленная версия ''torch''

Зайти в интерпретатор python (Note: выход из консольного интерпретатора по ''Ctrl+D'')

''python3''

Выполнить последовательно

'''
>>> import torch
'''
''>>> print(torch.cuda.is_available())''

Ожидаем: True

'''
>>> print(torch.cuda.get_device_name(0)) 
'''

Ожидаем: NVIDIA GeForce __Название_Видеокарты__

''>>> print(torch.version.cuda)'' 

Ожидаем: версию уже установленной CUDA из версии Nvidia — 12.2 

Если ответ другой (например, 12.4) — алярм. Из-за разницы в версиях ПО Whisper не сможет делать расчёты на видеокарте, увы, остаётся запускать Whisper только на CPU. См. [[+Пример неудачного решения]]

3)
Установить утилиту для мониторинга GPU

''sudo apt install nvtop''

После запуска Whisper вызвать в отдельной консоли ''nvtop'', а в другой ''htop'' и смотреть на графики. Если в окне ''nvtop'' нет явных скачков графика, значит что-то пошло не так.

Также во время работы Whisper можно сделать

''nvidia-smi''

и ожидаем увидеть в таблице строку вроде 

''|    0   N/A  N/A     27061      C   ...ace/whisper/whisper_env/bin/python3     5416MiB'' 

===== Использовать Whisper =====

Мой фреймворк на Bash: https://github.com/testitquickly/bystro.whisper

=== Запустить Whisper ===

Перейти в каталог с уже созданным (при установке Whisper) виртуальным окружением Python:

''cd ~/whisper/''

Активировать виртуальное окружение Python:

''source whisper_env/bin/activate''

Для выхода из виртуального окружения надо выполнить в том же каталоге команду “''deactivate''”.

==== Предварительная конвертация аудио ====

Whisper может читать mp3, но лучше подать ему монофонические файлы формата PCM (WAV) с дискретной частотой 16 кГц (можно и больше, но для записи голоса это избыточно).

Если файл записан в любом другом формате (от аудио в ''ogg'' или ''mp3'' до видео в ''mp4''), можно сперва конвертировать его через [[Debian:Install:FFmpeg]]:

''file="''__''ИмяФайла''__''"; ffmpeg -i"$file" -ar 16000 -ac 1 "${file%.*}.wav"''

Если надо конвертировать ВСЕ файлы в каталоге:

''for file in /''__''INPUT_FOLDER''__''/*.mp3; do ffmpeg -i "$file" -ar 16000 -ac 1 "${file%.mp3}.wav"; done''

Может быть удобнее обработать файлы из одного каталога и сохранить сконвертированные файлы //в другом// каталоге:

''for file in /''__''INPUT_FOLDER''__''/*.mp3; do ffmpeg -i "$file" -ar 16000 -ac 1 "/''__''OUTPUT_FOLDER''__''/$(basename "${file%.mp3}.wav")"; done''

=== Конвертация аудио через Audacity ===

Уместно, когда что-то надо вырезать.

В окне [[Audacity]] одна из панелек управления называется «Частота проекта (Гц)» — она же "Project Rate (Hz)". Установить ''16000''. Это глобальная настройка, которая задаёт частоту дискретизации всего проекта, а не отдельных файлов.

Если дорожка стерео — свести её в моно через меню Tracks > Mix > Mix Stereo down to Mono

Затем File → Export → Export as WAV

В появившемся окне: Save as type: WAV (Microsoft) signed 16-bit PCM

Перепроверить, что частота проекта всё ещё 16000 Hz и сохранить файл.

На выходе будет *.wav для Whisper — формат PCM 16-bit, mono, 16 kHz

==== Преобразовать в текст один аудиофайл ====

''whisper АУДИОФАЙЛ.mp3 --language Russian --model medium --device cuda''

Если указать ''--device cpu'' — вычисления должно происходить на CPU.

В каталоге, из которого выполняется эта команда, появится несколько файлов с расшифрованным текстом:

*.json
*.srt
*.tsv
*.txt
*.vtt

В *.json записывается какой-то RTF-подобный текст:

''{"text": " \u0441 \u0447\u0435\u0433\u043e \u043e\u0431\u044b\u0447\u043d\u043e \u043d\u0430\u0447\u0438\u043d\u0430\u0435\u0442\u0441\u044f…''

В *.txt текст записан очень неудобно для восприятия — сплошным потоком, иногда вообще без знаков препинания.

В *.vtt записан текст в виде формата субтитров WEBVTT (Web Video Text Tracks) — он используется в веб-приложениях и поддерживается HTML5. В файлах *.srt то же самое, только ещё и с нумерацией реплик. В принципе файл *.vtt — самый удобный для восприятия.

Чтобы не создавать множество текстовых файлов, можно указать Whisper нужный формат файла с текстом:

''whisper АУДИОФАЙЛ.mp3 --language Russian --model medium --device cuda --output_format vtt''

Если обрабатываемый «АУДИОФАЙЛ.mp3» находится в другом каталоге, то разумно сохранить расшифровку рядом с ним:

'''
FILE="/home/astenix/Аудио/1min.wav"; whisper $FILE --language Russian --model medium --device cuda --output_dir "$(dirname $FILE)" --output_format vtt
'''

Кавычки нужны на случай если в полном пути к файлу будут пробелы.

==== Преобразовать в текст множество аудиофайлов ====

Надо

* преобразовать последовательно все аудиофайлы, которые находятся в каталоге ''/home/astenix/Аудио'' 
* и подавить вывод текста в консоль, 
* и оставить уведомление о том, что выполняется очередная задача:

''FOLDER="/home/astenix/transcribere/input"; for FILE in $FOLDER/*.wav; do echo "Выполняется транскрибирование — $FILE"; whisper "$FILE" --language Russian --model medium --device cuda --output_dir "$(dirname "$FILE")" --output_format vtt > /dev/null 2>&1; done; echo -e "\n\tТранскрибирование завершено.\n"''

В переменной FOLDER не надо указывать закрывающий слэш.
