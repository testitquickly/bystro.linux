Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.6
Creation-Date: 2025-06-19T01:28:54+03:00

====== Sentence Transformer ======

[ @machine_learning @bert @SentenceTransformer ]

Транскрибированные [[Whisper]] тексты получаются нечитаемыми — все предложения слиты в один большой абзац с условными переносами, нет смысловой ясности и внятных абзацев. Всё равно приходится слушать аудиофайл и стараться хотя бы разбить текст на смысловые блоки. Тут может помочь “SentenceTransformer” — модель для получения смыслов из всего текста и отдельных предложений. 

Надо очень хорошо понимать, что “SentenceTransformer” не chatGPT, он не переосмысливает и не переписывает исходный текст. Он ищет условные смысловые блоки из отдельных предложений, и собирает их в отдельные крупные абзацы. Это не отменяет необходимости слушать аудиозаметку, но этого уже хорошо помогает лучше воспринимать текст.

===== Теория =====

**Числовой вектор** — это упорядоченный набор чисел, например [0.12, -0.87, 0.55, 0.01, ...] В машинном обучении и обработке текста вектор используется для представления неопределенных объектов (слов, предложений, изображений и т.п.) в определенном виде чисел. Компьютер слова не понимает, ему нужны только числа. Каждое число в числовом векторе — это координата в многомерном пространстве.

**Embedding** — встраивание, внедрение (англ.)

**Смысловой эмбеддинг предложения** — это числовой вектор, который представляет смысл этого предложения в виде набора чисел, которые понятны компьютеру. Чем ближе два таких вектора в многомерном пространстве, тем более похожи по смыслу предложения, которые они представляют. Например, предложения "Кошка сидит на ковре" и "Животное лежит на полу" будут иметь похожие эмбеддинги, потому что слова "кошка" и "животное" близки по смыслу. Смысловые эмбеддинги позволяют сравнивать предложения, находить похожие тексты, группировать их по смыслу и использовать в задачах поиска и рекомендаций.

**BERT** (Bidirectional Encoder Representations from Transformers) — языковая модель (не путать с ИИ, который основан на machine learning). Разработана Google в 2018 году. Это трансформер, который читает текст с обеих сторон сразу (слева направо и справа налево), чтобы лучше понимать контекст слов и предсказывать пропущенные слова и отношения между словами (общий контекст текста).

В 2019-м году на факультете Computer Science технического университете Дармштадта (Technische Universität Darmstadt, Германия) была представлена модель “SentenceTransformer” — улучшенный способ получения смысловых эмбеддингов предложений на основе BERT. Это адаптация гугловской модели BERT, которую оптимизировали для получения хороших эмбеддингов целиком для предложения/текста.

* Смысловой эмбеддинг предложения — числовое представление смысла всего предложения, чтобы сравнивать и анализировать тексты.
* BERT — модель для понимания контекста слов в тексте через смысловой эмбеддинг предложений.
* SentenceTransformer — улучшенный BERT (SBERT), общее название библиотеки и моделей, построенных на основе идеи Sentence-BERT. Выполняет семантический поиск по тексту и анализ парафраз ( переформулировка одного и того же смысла другими словами; два разных предложения считаются парафразами, если они говорят об одном и том же, но другими словами). SBERT помогает LLM (универсальная языковая модель, „chatGPT“) вычислять из текста отдельные смысла.

Официальный репозиторий: https://github.com/UKPLab/sentence-transformers

===== Установка =====

* [[Debian:Install:Sentence Transformer]]

===== Запуск =====

1. Запустить виртуальное окружение Python:

''source SentenceTransformer_env/bin/activate''

2. Вставить в файл ''output.txt'' текст, который нужно обработать.

3. Выполнить скрипт:

''python sentence-transformer.py''

4. Посмотреть в  ''output.txt:''

''mcedit output.txt''

===== Настройка =====

Вариантов этого трансформера множество. Не все они работают с русским языком.

лёгкая модель, быстрая, средний результат

''sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2''

оптимальна для CPU

''intfloat/multilingual-e5-base''

лучший результат, но очень тяжёлая, сильно нагружает CPU   

''intfloat/multilingual-e5-large''

Разумно начать с легкой и посмотреть, как пойдёт. Если результат не совсем хорош, можно в начале файла ''sentence-transformer.py'' поменять параметр ''eps_eval'', он же «чувствительность кластеризации», это разделение набора объектов на группы.
* Если почти каждое предложение текста оказалось на отдельном абзаце — eps надо увеличить, например, вместо 0,3 прописать 0,39 или 0,42
* Если предложения «закатаны» в большие абзацы, eps надо уменьшить, например, вместо 0,3 прописать 0,25 или 0,22

С этим параметром надо экспериментировать в зависимости от размера используемой модели. Для самой маленькой модели ''eps'' в диапазоне между 0,20 к 0,40 (условно). А для самой большой модели  ''eps'' надо выставить в меньшем диапазоне между 0,15 и 0,20. 
