Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.6
Creation-Date: 2022-02-23T21:11:45+02:00

====== wget ======

[ @wget @console ]

Скачиваем все файлы через "файл-лист" с помощью Wget

''wget -i'' http://www.example.site/filelist/your_list_here.urls ''--trust-server-names=on --restrict-file-names=nocontrol''

2
Вероятны траблы, если файлы именованы на русском языке. Тогда надо настроить Wget в обход --trust-server-names=on — в файле "~/.wgetrc" сделать так:

--------------------
'''
user-agent = «Mozilla/7.0»
tries = 10
wait = 1
continue = on
background = off
content_disposition = on
limit_rate = 10485760
restrict_file_names = nocontrol
'''

--------------------

Тогда закачка очень простая:

''wget -i your_list_here.urls''

3
Ручная версия:

1. Качаем your_list_here.urls
2. Открываем любимым редактором и перед каждой строчкой (это можно сделать простой заменой) вписываем без кавычек "wget -c --content-disposition "
3. Делаем файл *.url исполняемым и запускаем его.

Замену можно произвести автоматически, выполнив команды последовательно:

''sed 's/http/wget -c --content-disposition http/g' filename.url > ex2download.sh''
''chmod +x ex2download.sh''
''ex2download.sh''

===== Как скачать сайт целиком с помощью wget =====

''wget -r -k -p -l 2 -E -nc'' [[http://site.com/|''http://site.com/'']]

После выполнения данной команды в директорию site.com будет загружена локальная копия сайта http://site.com. Чтобы открыть главную страницу сайта нужно открыть файл index.html.

Рассмотрим используемые параметры:
* -r — указывает на то, что нужно рекурсивно переходить по ссылкам на сайте, чтобы скачивать страницы.
* -k — используется для того, чтобы wget преобразовал все ссылки в скаченных файлах таким образом, чтобы по ним можно было переходить на локальном компьютере (в автономном режиме).
* -p — указывает на то, что нужно загрузить все файлы, которые требуются для отображения страниц (изображения, css и т.д.).
* -l — определяет максимальную глубину вложенности страниц, которые wget должен скачать (по умолчанию значение равно 5, в примере мы установили 2). В большинстве случаев сайты имеют страницы с большой степенью вложенности и wget может просто «закопаться», скачивая новые страницы. Чтобы этого не произошло можно использовать параметр -l.
* -E — добавлять к загруженным файлам расширение .html.
* -nc — при использовании данного параметра существующие файлы не будут перезаписаны. Это удобно, когда нужно продолжить загрузку сайта, прерванную в предыдущий раз.
