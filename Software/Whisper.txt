Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.6
Creation-Date: 2024-11-30T21:46:21+02:00

====== Whisper ======

[ @sound @openai @whisper ]

«Whisper» — нейронная сеть, модель для автоматического распознавания речи — automatic speech recognition (ASR) от OpenAI (есть аналоги). Может преобразовывать аудиофайлы в текст (это называется «транскрибирование»). Работает локально. Опубликована с открытым исходным кодом. Поддерживает множество языков (русский и украинский) под «MIT License».

Основные сценарии её применения

1. Транскрибация аудио и видео — расшифровка интервью, подкастов, лекций и др. Создание субтитров
2. Голосовые помощники и чат-боты
3. Автоматизация работы с аудиоархивами — распознавание и индексирование больших аудиоколлекций и «поиск по аудиофайлам» на основе полученного текста
4. Перевод речи в текст в реальном времени
5. Лингвистический анализ и исследование речи через исследования в области диалектологии и фонетики

Whisper поддерживает много языков и обладает почти высокой точностью транскрибирования англоязычной речи в текст, особенно при хорошем качестве записи.

Описание: https://openai.com/index/whisper/

===== Условности =====

Нейросети в работе вообще прожорливы, на большом количестве файлов с применением «большой» модели гудеть кулерами начнёт даже ноутбук, в котором в принципе нет вентиляторов. Разумно начинать с модели „medium” и переходить на более продвинутую только при явной необходимости. Также из сострадания к ресурсам компьютера разумно придирчиво отбирать, какие аудиофайлы действительно //надо// транскрибировать. И в принципе надо сперва настроить то, как Whisper будет использовать CPU (см. файл про его установку). 

У Whisper есть несколько моделей (уже натренированные нейросети):

* //tiny// и //base//: работают быстро, но точность распознавания очень, эээ, плохая.
* //small//: малая точность, но работает шустро. 
* //__medium__//: хороший баланс между точностью и скоростью.
* //large//: высокая точность, но требует много ресурсов (сильно разгоняет даже очень мощный процессор). Использовать на большом наборе файлов не надо, после 30 минут транскрибации процессор разгонался до 80 по Цельсию, было стрёмно. Для распознавания голосовых бормоталок в офисных условиях избыточна.
* //turbo//: потребляет меньше ресурсов, чем модель //large//, но заточена под скорость, а не под точность расшифровки.

Модель „small” на момент тестирования весит 461 МБ. Модель „large” [[https://github.com/openai/whisper/blob/main/README.md#available-models-and-languages|потребует]] скачать почти три гигабайта. Позже будет больше.

Каждая модель Whisper перед первым её применением будет автоматически загружена из сети и сохранена в кэше профиля пользователя: ''~/.cache/whisper/'' в файле с расширением ''.pt''

===== Установить Whisper =====

* [[Debian:Install:Whisper]]

===== Настроить Whisper =====

Whisper нагружает CPU очень агрессивно, на уровне 97-101%, даже если ему выделить только одно ядро процессора. Это разгоняет ВЕСЬ камень до +80°C, независимо от системы его охлаждения. Для компьютера в крупном корпусе это не так, чтобы «ой мама!», но всё-таки… и вентиляторы загудят. А для тесного ноутбука это прям заметно много. 

Можно «замедлить» Whisper комплексно:
* использовать модели tiny или small
* ограничить количество задействованных ядер процессора (3 штуки)
* ограничить число потоков для библиотек нейросети (BLAS, MKL, OpenMP)
* отключить вычисления в формате float16 если расчет происходит только на CPU

Модели Tiny или Small хуже распознают речь, поэтому нет, оставим Medium. Остальные ограничения заметно придушат производительность нейросети, но это означает, что Whisper всего лишь будет работать чуть дольше, и это ок. Важно то, что эти настройки снизили рабочую температуру процессора с +88°C до +70°C. 

==== Работа на CPU ====

У процессора может быть несколько физических ядер (threads), и над ними несколько виртуальных (hyperthreads). В настройке Whisper подразумевается использование //только// физических ядер.

Узнать, сколько всего ядер на процессоре компьютера:

'''
nproc
'''

Пример ответа: ''16''

Узнать, сколько физических ядер: 

''cat /proc/cpuinfo | grep 'core id' | sort -u | wc -l''

или

''grep -m1 'cpu cores' /proc/cpuinfo''

Пример ответа: ''8''

Задать количество используемых физических ядер процессора:

''--threads 3''

Чем меньше — тем тише (и дольше) будет работать Whisper.

=== Ограничить число потоков вычислений ===

Whisper использует PyTorch, который может задействовать BLAS, MKL, OpenMP — технологии для ускорения вычислений в математических и научных задачах. Это ускоряет обработку, но может занять все ядра процессора, вызывая перегрузку.

1. BLAS (Basic Linear Algebra Subprograms) — библиотека базовых операций линейной алгебры (матрицы, векторы).
2. MKL (Math Kernel Library) — библиотека Intel, включает BLAS, LAPACK, FFT и другие алгоритмы.
3. OpenMP (Open Multi-Processing) — API для параллельного выполнения кода на CPU. Позволяет использовать многоядерность для ускорения вычислений.

Проверить их настройки по-умолчанию:

''echo $OPENBLAS_NUM_THREADS; echo $MKL_NUM_THREADS; echo $OMP_NUM_THREADS; echo $NUMEXPR_NUM_THREADS''

Если в ответ пустота, это значит, что Whisper будет использовать их без ограничений.

Можно регулировать многопоточность этих вычислений через объявление количества используемых ядер в четырех переменных, например „''2''”. 

Задать „''1''” для принудительного перевода в режим однопоточности:

'''
export OPENBLAS_NUM_THREADS=1
export MKL_NUM_THREADS=1
export OMP_NUM_THREADS=1
export NUMEXPR_NUM_THREADS=1
'''

=== Отключить вычисления в формате float16 ===

Это нужно, только если транскрибирование выполняется только на CPU.

Добавить в цепочку настроек Whisper при запуске параметр

''--fp16 False'' 

==== Работа на GPU ====

Очень разумно выполнять всю работу Whisper на относительно свежей видеокарте от Nvidia.

Но для этого

1)
Должны быть установлены драйверы для установленной видеокарты — [[Debian:Install:Nvidia]]

И должна быть известна версия CUDA (Compute Unified Device Architecture — программно-аппаратная архитектура параллельных вычислений, работает на графических процессорах Nvidia), которая совместима с установленной системой

''nvidia-smi''

В ответе посмотреть верхнюю строку:

''Driver Version: 535.216.01   CUDA Version: 12.2'' 

2)
Проверить, какие версии CUDA поддерживает установленная версия ''torch''

Зайти в интерпретатор python (Note: выход из консольного интерпретатора по ''Ctrl+D'')

''python3''

Выполнить последовательно

'''
>>> import torch
'''
''>>> print(torch.cuda.is_available())''

Ожидаем: True

'''
>>> print(torch.cuda.get_device_name(0)) 
'''

Ожидаем: NVIDIA GeForce __GTX 1050 Ti__ (пример названия установленной видеокарты; конкретно это уже неподходящая для Whisper)

''>>> print(torch.version.cuda)'' 

Ожидаем: версию уже установленной CUDA из версии Nvidia — 12.2 

Если ответ другой (например, 12.4) — это означает, что из-за разницы в версиях ПО Whisper не сможет делать расчёты на видеокарте. Скорее всего причина в том, что видеокарта «постаревшая». Так или иначе, если такая ситуация выявлена — рекомендую прекратить трепыхаться и начать запускать Whisper на CPU. Увы. См. [[+Пример неудачного решения]]

3)
Затем установить утилиту для мониторинга GPU

''sudo apt install nvtop''

После запуска Whisper вызвать в отдельной консоли ''nvtop'', а в другой ''htop'' и смотреть на графики. Если в окне ''nvtop'' нет явных скачков графика, значит что-то пошло не так.

Также во время работы Whisper можно сделать

''nvidia-smi''

и ожидаем увидеть в таблице строку вроде 

''|    0   N/A  N/A     27061      C   ...ace/whisper/whisper_env/bin/python3     5416MiB'' 

===== Использовать Whisper =====

Я собрал на Bash фреймворк для запуска транскрибирования аудофайлов с речевыми заметками:  https://github.com/testitquickly/bystro.whisper

=== Запустить Whisper ===

Перейти в каталог с уже созданным (при установке Whisper) виртуальным окружением Python:

''cd ~/whisper/''

Активировать виртуальное окружение Python:

''source whisper_env/bin/activate''

Для выхода из виртуального окружения надо выполнить в том же каталоге команду “''deactivate''”.

==== Предварительная конвертация аудио ====

Whisper может читать mp3, но лучше подать ему файлы формата PCM (WAV) с дискретной частотой 16 кГц (можно и больше, но для записи голоса это избыточно, и процессор будет излишне напрягаться).

Если файл записан в любом другом формате (от аудио в ''ogg'' или ''mp3'' до видео в ''mp4''), можно сперва конвертировать его через [[Debian:Install:FFmpeg]]:

''file="''__''ИмяФайла''__''"; ffmpeg -i"$file" -ar 16000 -ac 1 "${file%.*}.wav"''

Если надо конвертировать ВСЕ mp3-файлы в каталоге:

''for file in /''__''INPUT_FOLDER''__''/*.mp3; do ffmpeg -i "$file" -ar 16000 -ac 1 "${file%.mp3}.wav"; done''

Может быть удобнее обработать файлы из одного каталога и сохранить сконвертированные файлы //в другом// каталоге:

''for file in /''__''INPUT_FOLDER''__''/*.mp3; do ffmpeg -i "$file" -ar 16000 -ac 1 "/''__''OUTPUT_FOLDER''__''/$(basename "${file%.mp3}.wav")"; done''

==== Преобразовать в текст один аудиофайл ====

''whisper АУДИОФАЙЛ.mp3 --language Russian --model medium --device cuda''

Если указать ''--device cpu'' — вычисления должно происходить на CPU.

В каталоге, из которого выполняется эта команда, появится несколько файлов с расшифрованным текстом:
*.json
*.srt
*.tsv
*.txt
*.vtt

В *.json записывается какой-то RTF-подобный текст:

''{"text": " \u0441 \u0447\u0435\u0433\u043e \u043e\u0431\u044b\u0447\u043d\u043e \u043d\u0430\u0447\u0438\u043d\u0430\u0435\u0442\u0441\u044f…''

В *.txt текст записан очень неудобно для восприятия — сплошным потоком, иногда вообще без знаков препинания.

В *.vtt записан текст в виде формата субтитров WEBVTT (Web Video Text Tracks) — он используется в веб-приложениях и поддерживается HTML5. В файлах *.srt то же самое, только ещё и с нумерацией реплик. В принципе файл *.vtt — самый удобный для восприятия.

Чтобы не создавать множество текстовых файлов, можно указать Whisper нужный формат файла с текстом:

''whisper АУДИОФАЙЛ.mp3 --language Russian --model medium --device cuda --output_format vtt''

Если обрабатываемый «АУДИОФАЙЛ.mp3» находится в другом каталоге, то разумно сохранить расшифровку рядом с ним:

'''
FILE="/home/astenix/Аудио/1min.wav"; whisper $FILE --language Russian --model medium --device cuda --output_dir "$(dirname $FILE)" --output_format vtt
'''

Кавычки нужны на случай если в полном пути к файлу будут пробелы.

==== Преобразовать в текст множество аудиофайлов ====

Тут надо
* преобразовать последовательно все аудиофайлы, которые находятся в каталоге ''/home/astenix/Аудио'' 
* и подавить вывод текста в консоль, 
* и оставить уведомление о том, что выполняется очередная задача:

''FOLDER="/home/astenix/transcribere/input"; for FILE in $FOLDER/*.wav; do echo "Выполняется транскрибирование — $FILE"; whisper "$FILE" --language Russian --model medium --device cuda --output_dir "$(dirname "$FILE")" --output_format vtt > /dev/null 2>&1; done; echo -e "\n\tТранскрибирование завершено.\n"''

В переменной FOLDER не надо указывать закрывающий слэш.
